{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dataloader import load_quora\n",
    "from heckman import weight_generating\n",
    "import numpy as np\n",
    "from dataloader import load_artificial_dataset\n",
    "from math import fabs\n",
    "from scipy.stats import norm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import time\n",
    "\n",
    "\n",
    "def save_weights(name, weight):\n",
    "    np.save(open(\"./weights/weights_for_\" + name + \".npy\", \"wb\"), weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Loading QuoraQP ----------\n",
      "Success!\n",
      "---------- Generating weights ----------\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: nan\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.407722\n",
      "         Iterations 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing inverse Mills ratios: 293761it [02:24, 2036.57it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-6-9a30af5edd54>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mquora_data\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mload_quora\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mweights\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mweight_generating\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mquora_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mquora_data\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0msave_weights\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'quoraqp'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Files\\heckman_debias_model\\weight_generating\\heckman.py\u001B[0m in \u001B[0;36mweight_generating\u001B[1;34m(x, y, param)\u001B[0m\n\u001B[0;32m     40\u001B[0m             \u001B[0mtmp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     41\u001B[0m             \u001B[0mtmp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mlist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0miloc\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mcount\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 42\u001B[1;33m             \u001B[0mIM\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdot\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtmp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mProbit_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mparams\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     43\u001B[0m             \u001B[0mIM_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mIM\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     44\u001B[0m     \u001B[0mweight\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrue_divide\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mIM_list\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmean\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mIM_list\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Files\\heckman_debias_model\\weight_generating\\heckman.py\u001B[0m in \u001B[0;36mim\u001B[1;34m(param)\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0mim\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrue_divide\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnorm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpdf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnorm\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcdf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\86155\\anaconda3\\envs\\heckman_debias_model\\lib\\site-packages\\scipy\\stats\\_distn_infrastructure.py\u001B[0m in \u001B[0;36mpdf\u001B[1;34m(self, x, *args, **kwds)\u001B[0m\n\u001B[0;32m   1767\u001B[0m             \u001B[0mgoodargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0margsreduce\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcond\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m+\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mscale\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1768\u001B[0m             \u001B[0mscale\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgoodargs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mgoodargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgoodargs\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1769\u001B[1;33m             \u001B[0mplace\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0moutput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcond\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_pdf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0mgoodargs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m/\u001B[0m \u001B[0mscale\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1770\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1771\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0moutput\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\86155\\anaconda3\\envs\\heckman_debias_model\\lib\\site-packages\\numpy\\lib\\function_base.py\u001B[0m in \u001B[0;36mplace\u001B[1;34m(arr, mask, vals)\u001B[0m\n\u001B[0;32m   1723\u001B[0m                         \"not {name}\".format(name=type(arr).__name__))\n\u001B[0;32m   1724\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1725\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_insert\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmask\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvals\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1726\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1727\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "quora_data = load_quora()\n",
    "weights = weight_generating(quora_data[0], quora_data[1], 1)\n",
    "save_weights('quoraqp', weights)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Loading artificial dataset ----------\n",
      "Success!\n"
     ]
    }
   ],
   "source": [
    "loc = 4\n",
    "scale = 4\n",
    "artificial_data = load_artificial_dataset()\n",
    "weights_list = [np.true_divide(norm.pdf(i, loc, scale), norm.cdf(i, loc, scale)) for i in artificial_data[0]]\n",
    "weights_list = np.array(weights_list) / np.mean(np.array(weights_list))\n",
    "save_weights('artificial_dataset', weights_list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "from keras.layers import Embedding\n",
    "max_nb_words = 50000\n",
    "emb_dim = 300\n",
    "max_seq_len = 35\n",
    "tr = pd.read_csv('./quora/train.tsv', delimiter='\\t', header=None)\n",
    "tr.columns = ['is_duplicate', 'question1', 'question2', 'pair_id']\n",
    "tr = tr[['is_duplicate', 'question1', 'question2']]\n",
    "\n",
    "val = pd.read_csv('./quora/dev.tsv', delimiter='\\t', header=None)\n",
    "val.columns = ['is_duplicate', 'question1', 'question2', 'pair_id']\n",
    "val = val[['is_duplicate', 'question1', 'question2']]\n",
    "\n",
    "tst = pd.read_csv('./quora/test.tsv', delimiter='\\t', header=None)\n",
    "tst.columns = ['is_duplicate', 'question1', 'question2', 'pair_id']\n",
    "tst = tst[['is_duplicate', 'question1', 'question2']]\n",
    "data = pd.concat([tr, val, tst], sort=False).fillna('')\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    text = re.sub('[^A-Za-z0-9]', ' ', text.lower())\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "\n",
    "data['question1'] = data['question1'].apply(text_cleaning)\n",
    "data['question2'] = data['question2'].apply(text_cleaning)\n",
    "tokenizer = Tokenizer(num_words=max_nb_words, oov_token='oov_token_placeholder')\n",
    "tokenizer.fit_on_texts(list(data['question1'].values) + list(data['question2'].values))\n",
    "sequences_1 = tokenizer.texts_to_sequences(data['question1'].values)\n",
    "sequences_2 = tokenizer.texts_to_sequences(data['question2'].values)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "word2vec = {}\n",
    "fin = io.open('./quora/wordvec.txt', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "for line in fin:\n",
    "    tokens = line.rstrip().split(' ')\n",
    "    word2vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.keys()))\n",
    "print('Preparing embedding matrix')\n",
    "nb_words = min(max_nb_words, len(word_index))\n",
    "emb = np.zeros((nb_words + 1, emb_dim))\n",
    "miss_cnt = 0\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        break\n",
    "    if word in word2vec.keys():\n",
    "        emb[i] = word2vec[word]\n",
    "    else:\n",
    "        emb[i] = (np.random.rand(emb_dim) - 0.5) * 0.1\n",
    "        miss_cnt += 1\n",
    "print('Null word embeddings: %d' % miss_cnt)\n",
    "\n",
    "embedding_layer = Embedding(emb.shape[0],\n",
    "                                emb.shape[1],\n",
    "                                weights=[emb],\n",
    "                                trainable=False,\n",
    "                                input_length=max_seq_len)\n",
    "embedding_layer(data['question1'], data['question2'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = np.array([artificial_data[0]]).T\n",
    "y = artificial_data[1]\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=10000, max_depth=9, random_state=2018, n_jobs=8, criterion='entropy')\n",
    "time_start=time.time()\n",
    "y_pred = cross_val_predict(clf, X, y, cv=100, method='predict_proba', verbose=3, n_jobs=3)\n",
    "time_end = time.time()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "propensity = np.array([y_pred[i, int(y[i])] for i in range(len(y))])\n",
    "prob_1_l = np.array([(propensity[i] if y[i] == 1 else (1-propensity[i]))\n",
    "          for i in range(len(y))])\n",
    "prob_0_l = 1 - prob_1_l\n",
    "\n",
    "\n",
    "def calculate_weight_fraction(prob_1):\n",
    "    prob_0 = 1 - prob_1\n",
    "    w1 = 1 / (prob_0 * prob_1_l / (prob_0 * prob_1_l + prob_1 * prob_0_l))\n",
    "    w0 = 1 / (prob_1 * prob_0_l / (prob_0 * prob_1_l + prob_1 * prob_0_l))\n",
    "    return sum(w1[i] for i in range(len(y)) if y[i] == 1) / sum(w0[i] for i in range(len(y)) if y[i] == 0)\n",
    "\n",
    "\n",
    "prior_fraction = np.sum(y) / (len(y) - np.sum(y))\n",
    "l, r = 0, 1\n",
    "thr = 0.00000000001\n",
    "step = 100\n",
    "# while l + thr < r:\n",
    "for _ in range(step):\n",
    "    m1 = l + (r- l) / 2\n",
    "    if calculate_weight_fraction(m1) < prior_fraction:\n",
    "        l = m1\n",
    "    else:\n",
    "        r = m1\n",
    "\n",
    "m0 = 1 - m1\n",
    "w1 = 1 / (m0 * prob_1_l / (m0 * prob_1_l + m1 * prob_0_l))\n",
    "w0 = 1 / (m1 * prob_0_l / (m0 * prob_1_l + m1 * prob_0_l))\n",
    "weights = np.array([(w1[i] if y[i] == 1 else w0[i]) for i in range(len(y))])\n",
    "assert fabs(prior_fraction - sum([weights[i] for i in range(len(y)) if y[i] == 1]) / sum([weights[i] for i in range(len(y)) if y[i] == 0])) < 0.0001\n",
    "weights = np.true_divide(weights, np.mean(weights))\n",
    "np.save(open(\"./weights/weights_artificial_random_forest.npy\", \"wb\"), weights)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}